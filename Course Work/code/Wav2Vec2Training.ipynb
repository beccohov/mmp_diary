{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n# Установка используемых библиотек и настройка окружения\n!pip install git+https://github.com/huggingface/datasets.git\n!pip install git+https://github.com/huggingface/transformers.git\n!pip install jiwer\n!pip install torchaudio\n\n%env LC_ALL=C.UTF-8\n%env LANG=C.UTF-8\n%env TRANSFORMERS_CACHE=/content/cache\n%env HF_DATASETS_CACHE=/content/cache\n%env CUDA_LAUNCH_BLOCKING=1","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:26:06.687317Z","iopub.execute_input":"2022-05-25T09:26:06.687713Z","iopub.status.idle":"2022-05-25T09:27:43.679515Z","shell.execute_reply.started":"2022-05-25T09:26:06.687615Z","shell.execute_reply":"2022-05-25T09:27:43.678252Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"AUGMENTED = True # Использовать ли аугментацию\nALPHA = 0.3\nDATASET = 'ravdess' # Либо \"emodb\", \"cremad\" \nFOLD = 0 # 0, 1, 2 ...N, где N - количество фолдов в соответствии с работой \nEPOCHS = 10 # Можно изменять количество эпох, в работе указаны использованные значения \nRAVDESS = # Ваш путь до директории здесь\nEMODB = # Ваш путь до директории здесь\nCREMAD = # Ваш путь до директории здесь","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:27:43.681877Z","iopub.execute_input":"2022-05-25T09:27:43.682208Z","iopub.status.idle":"2022-05-25T09:27:43.688402Z","shell.execute_reply.started":"2022-05-25T09:27:43.682166Z","shell.execute_reply":"2022-05-25T09:27:43.687341Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport torchaudio\nfrom sklearn.model_selection import train_test_split\nimport torchaudio\nimport IPython.display as ipd\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple\nimport torch\nfrom torch import nn\nfrom transformers.file_utils import ModelOutput\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Union\nimport transformers\nfrom transformers import Wav2Vec2Processor, EvalPrediction\nfrom typing import Any, Dict, Union\nfrom packaging import version\nfrom torch.utils.data import DataLoader\n\nimport scipy.io.wavfile as wf\nfrom torch.utils.data import Dataset\nfrom transformers import AutoConfig, Wav2Vec2Processor, TrainingArguments, Trainer, is_apex_available\n\n\nfrom datasets import load_dataset, load_metric\nimport glob\nimport torch\nimport torch.nn as nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.models.wav2vec2.modeling_wav2vec2 import (\n    Wav2Vec2PreTrainedModel,\n    Wav2Vec2Model\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:27:43.689770Z","iopub.execute_input":"2022-05-25T09:27:43.690431Z","iopub.status.idle":"2022-05-25T09:27:52.929599Z","shell.execute_reply.started":"2022-05-25T09:27:43.690386Z","shell.execute_reply":"2022-05-25T09:27:52.928534Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Перевод RAVDESS в датафрейм для удобства дальнейшей работы\nencode_emotions = { \n    1 : 'neu',\n    2 : 'calm', \n    3 : 'hap',\n    4 : 'sad',\n    5 : 'ang',\n    6: 'fea',\n    7: 'dis',\n    8: 'sup'\n}\ndataset_data = []\nfor path in glob.glob(RAVDESS+'/Actor_*/*.wav'):\n    filename = path.split('/')[-1][:-4]\n    modality, vocal_channel, emotion, emo_intensity, statement, repetition, actor = [int(x) for x in filename.split('-')]\n    dataset_data.append([actor, statement, encode_emotions[emotion], 'ravdess', path])\ndf_ravdess = pd.DataFrame(dataset_data, columns = ['actor', 'statement', 'emotion', 'dataset', 'path'])","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:27:52.931808Z","iopub.execute_input":"2022-05-25T09:27:52.932119Z","iopub.status.idle":"2022-05-25T09:27:53.374821Z","shell.execute_reply.started":"2022-05-25T09:27:52.932080Z","shell.execute_reply":"2022-05-25T09:27:53.373963Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Перевод EMODB в датафрейм для удобства дальнейшей работы\nencode_emotions = {\n    'W' : 'ang',\n    'L' : 'bor', # == 'calm' ?\n    'E' : 'dis',\n    'A' : 'fea',\n    'F' : 'hap',\n    'T' : 'sad',\n    'N' : 'neu'\n}\ndataset_data = []\nfor file in glob.glob(EMODB+'/*.wav'):\n    filename = file[-11:-4]\n    speaker = filename[:2]\n    text_id = filename[3:5]\n    emotion_code = filename[5]\n    version_ = filename[6]\n    dataset_data.append([int(speaker), int(text_id), encode_emotions[emotion_code], 'emodb', file])\ndf_emodb = pd.DataFrame(dataset_data, columns = ['actor', 'statement', 'emotion', 'dataset', 'path'])","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:27:53.376172Z","iopub.execute_input":"2022-05-25T09:27:53.376479Z","iopub.status.idle":"2022-05-25T09:27:53.536949Z","shell.execute_reply.started":"2022-05-25T09:27:53.376443Z","shell.execute_reply":"2022-05-25T09:27:53.536109Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Перевод CREMA-D в датафрейм для удобства дальнейшей работы\ndataset_data = []\nfor path in glob.glob(CREMAD+'/*.wav'):\n    filename = path.split('/')[-1][:-4]\n    emotion = filename.split('_')[2]\n    actor = filename[:4]\n    dataset_data.append([int(actor)-1000,0, emotion.lower(), 'cremad', path])\n    \ndf_cremad = pd.DataFrame(dataset_data, columns = ['actor', 'statement', 'emotion', 'dataset', 'path'])","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:27:53.538552Z","iopub.execute_input":"2022-05-25T09:27:53.539585Z","iopub.status.idle":"2022-05-25T09:27:53.809505Z","shell.execute_reply.started":"2022-05-25T09:27:53.539529Z","shell.execute_reply":"2022-05-25T09:27:53.808470Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"if DATASET == 'emodb':\n    df = df_emodb\nelif DATASET == 'ravdess':\n    df = df_ravdess \nelif DATASET == 'cremad':\n    df = df_cremad\n\nelse:\n    raise NotImplementedError(\"Undefined Dataset name\")","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:27:53.811087Z","iopub.execute_input":"2022-05-25T09:27:53.811475Z","iopub.status.idle":"2022-05-25T09:27:53.818417Z","shell.execute_reply.started":"2022-05-25T09:27:53.811422Z","shell.execute_reply":"2022-05-25T09:27:53.817283Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Краткое описание данных\nprint(\"Labels: \", df[\"emotion\"].unique())\ndf.groupby(\"emotion\").count()[[\"path\"]]","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:27:53.819845Z","iopub.execute_input":"2022-05-25T09:27:53.820252Z","iopub.status.idle":"2022-05-25T09:27:53.861891Z","shell.execute_reply.started":"2022-05-25T09:27:53.820196Z","shell.execute_reply":"2022-05-25T09:27:53.860821Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"idx = np.random.randint(0, len(df))\nsample = df.iloc[idx]\npath = sample[\"path\"]\nlabel = sample[\"emotion\"]\n\n\nprint(f\"Расположение файла: {idx}\")\nprint(f\"      Эмоция: {label}\")\nprint()\n\nspeech, sr = torchaudio.load(path)\nspeech = speech[0].numpy().squeeze()\nipd.Audio(data=np.asarray(speech), autoplay=True, rate=sr)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:27:53.863789Z","iopub.execute_input":"2022-05-25T09:27:53.864359Z","iopub.status.idle":"2022-05-25T09:27:53.941471Z","shell.execute_reply.started":"2022-05-25T09:27:53.864309Z","shell.execute_reply":"2022-05-25T09:27:53.940292Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# За основу кода этой ячейки взят код https://github.com/m3hrdadfi/soxan с некоторыми изменениями и упрощениями\nidx = 0\n# Класс для хранения результатов\n@dataclass\nclass SpeechClassifierOutput(ModelOutput):\n    loss: Optional[torch.FloatTensor] = None\n    logits: torch.FloatTensor = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None\n\n# Классификатор для всей модели\nclass Wav2Vec2ClassificationHead(nn.Module):\n    \"\"\"Head for wav2vec classification task.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        global num_labels\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dropout = nn.Dropout(config.final_dropout)\n        self.out_proj = nn.Linear(config.hidden_size, num_labels)#config.num_labels\n        self.num_labels = num_labels\n    def forward(self, features, **kwargs):\n        x = features\n        x = self.dropout(x)\n        x = self.dense(x)\n        x = torch.tanh(x)\n        x = self.dropout(x)\n        x = self.out_proj(x)\n        return x\n\n# Интерфейс - модель, использующая Wav2Vec2\nclass Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n    def __init__(self, config):\n        global num_labels\n        super().__init__(config)\n        self.num_labels = num_labels\n        self.pooling_mode = \"mean\" # mean работает лучше\n        self.config = config\n\n        self.wav2vec2 = Wav2Vec2Model(config) # Сама модель\n        self.classifier = Wav2Vec2ClassificationHead(config) # Классификатор\n\n        self.init_weights()\n\n    def freeze_feature_extractor(self):\n        self.wav2vec2.feature_extractor._freeze_parameters()\n\n    def merged_strategy(self, hidden_states, mode=\"mean\"): # mean работает лучше\n        if mode == \"mean\":\n            outputs = torch.mean(hidden_states, dim=1)\n        elif mode == \"sum\":\n            outputs = torch.sum(hidden_states, dim=1)\n        elif mode == \"max\":\n            outputs = torch.max(hidden_states, dim=1)[0]\n        return outputs\n\n    def forward(self, input_values, attention_mask=None, output_attentions=None, output_hidden_states=None, \n                return_dict=None, labels=None ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        outputs = self.wav2vec2(\n            input_values,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = outputs[0] # берем последнее\n        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode) # Усредняем по времени\n        logits = self.classifier(hidden_states)\n        # Далее выдаем результат в нужном формате\n        loss = None\n        if labels is not None:\n            loss_fct = BCEWithLogitsLoss()\n            loss = loss_fct(logits, labels)\n        \n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SpeechClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n    \n\n# Словарь с точностью (можно добавлять другие метрики)\ndef compute_metrics(p: EvalPrediction):\n    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n    preds = np.argmax(preds, axis=1)\n    return {\"accuracy\": (preds == np.argmax(p.label_ids, 1)).astype(np.float32).mean().item()}\n\n\n\n\n\nif is_apex_available(): # апекс можно использовать, если есть поддержка.\n    # Вычисления будут эффективнее, меньше расход памяьти\n    from apex import amp\n\nif version.parse(torch.__version__) >= version.parse(\"1.6\"):\n    _is_native_amp_available = True\n    from torch.cuda.amp import autocast\n\n    \n    \nclass CTCTrainer(Trainer):\n    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n        \"\"\"\n        Класс-обертка для обучения модели, совместим с huggingface trainer, что делает удобным его применение\n        \"\"\"\n\n        model.train()\n        inputs = self._prepare_inputs(inputs)\n\n        if self.use_amp:\n            with autocast():\n                loss = self.compute_loss(model, inputs)\n        else:\n            loss = self.compute_loss(model, inputs)\n\n        if self.args.gradient_accumulation_steps > 1:\n            loss = loss / self.args.gradient_accumulation_steps\n\n        if self.use_amp:\n            self.scaler.scale(loss).backward()\n        elif self.use_apex:\n            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                scaled_loss.backward()\n        elif self.deepspeed:\n            self.deepspeed.backward(loss)\n        else:\n            loss.backward()\n\n        return loss.detach()\n\n# Загрузка и ресэмплинг при необходимости - используется сэмплинг рейт 16 000\n# (во всех аудиозаписях датасета используется тоже 16 000)\ndef speech_file_to_array_fn(path):\n    speech_array, sampling_rate = torchaudio.load(path)\n    if len(speech_array.shape) > 1:\n        speech_array = speech_array[0] # for two-channels audio\n    resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)\n    speech = resampler(speech_array).squeeze().numpy()\n    return speech\n\n# По метке получаем  число\ndef label_to_id(label, label_list):\n\n    if len(label_list) > 0:\n        return label_list.index(label) if label in label_list else -1\n\n    return label\n\n# В этой функции используются предыдущие две - загружается запись и метки преобразуются в число\ndef preprocess_function(examples):\n    speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]\n    target_list = [label_to_id(label, label_list) for label in examples[output_column]]\n    try:\n        result = {}\n        result = processor(speech_list, sampling_rate=target_sampling_rate)\n        result[\"labels\"] = list(target_list)\n    except:\n        print(\"error\")\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:27:53.944730Z","iopub.execute_input":"2022-05-25T09:27:53.945082Z","iopub.status.idle":"2022-05-25T09:27:53.985518Z","shell.execute_reply.started":"2022-05-25T09:27:53.945040Z","shell.execute_reply":"2022-05-25T09:27:53.984494Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass DataCollatorCTCWithPadding:\n    processor: Wav2Vec2Processor\n    padding: Union[bool, str] = True\n    max_length: Optional[int] = None\n    max_length_labels: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    pad_to_multiple_of_labels: Optional[int] = None\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        input_features = [{\"input_values\": torch.tensor(feature[\"input_values\"].astype('float'))} for feature in features]\n        label_features = [torch.nn.functional.one_hot(torch.tensor(feature[\"labels\"]), 8) if type(feature[\"labels\"]) == int else feature[\"labels\"] for feature in features]\n        \n        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n\n        batch = self.processor.pad(\n            input_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"pt\",\n        )\n        batch[\"labels\"] = torch.stack(label_features)\n        return batch","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:27:53.987256Z","iopub.execute_input":"2022-05-25T09:27:53.987603Z","iopub.status.idle":"2022-05-25T09:27:54.002225Z","shell.execute_reply.started":"2022-05-25T09:27:53.987555Z","shell.execute_reply":"2022-05-25T09:27:54.001394Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"pooling_mode = \"mean\"\nmodel_name_or_path = \"jonatasgrosman/wav2vec2-large-xlsr-53-english\"\ninput_column = \"path\"\noutput_column = \"emotion\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nOUT_DIR = f\"wav2vec2_fold{FOLD}\"","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:27:54.003848Z","iopub.execute_input":"2022-05-25T09:27:54.004995Z","iopub.status.idle":"2022-05-25T09:27:54.015015Z","shell.execute_reply.started":"2022-05-25T09:27:54.004918Z","shell.execute_reply":"2022-05-25T09:27:54.014262Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"if DATASET == 'ravdess':\n    # RAVDESS\n    actors_per_fold = {\n            0: [1, 2, 3, 4, 5],\n            1: [6, 7, 8, 9, 10],\n            2: [11, 12, 13, 14, 15],\n            3: [16, 17, 18, 19, 20],\n            4: [21, 22, 23, 24]\n    }\nelif DATASET == 'cremad':\n    # CREMAD\n    actors_per_fold = {\n            0: [10, 13, 14, 17, 18, 21, 26, 30, 32, 40, 41, 51, 58, 86],\n            1: [23, 24, 25, 27, 28, 29, 31, 33, 34, 35, 36, 37, 38, 39]\n    }\nelif DATASET == 'EMODB':\n    # EMODB\n    actors_per_fold = {\n        0: [13, 14, 15, 16],\n        1: [3, 8, 9] # Суммарное количество записей такое же, как в случае предыдущего фолда\n    }\nelse:\n    raise NotImplementedError(\"Undefined dataset\")","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:27:54.017049Z","iopub.execute_input":"2022-05-25T09:27:54.017789Z","iopub.status.idle":"2022-05-25T09:27:54.029194Z","shell.execute_reply.started":"2022-05-25T09:27:54.017736Z","shell.execute_reply":"2022-05-25T09:27:54.028355Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"\ntraining_args = TrainingArguments(\noutput_dir=OUT_DIR,\n\nper_device_train_batch_size=3,\nper_device_eval_batch_size=3,\ngradient_accumulation_steps=2,\nevaluation_strategy=\"steps\",\nnum_train_epochs=EPOCHS,\nfp16=(True if device =='cuda' else False),\nsave_steps=500,\neval_steps=20,\nlogging_steps=10,\nlearning_rate=1e-4,\nsave_total_limit=2,\n)\ntest_part = df.query(f'actor in {actors_per_fold[FOLD]}')\ntrain_part = df.query(f'not (actor in {actors_per_fold[FOLD]})')\nsave_path = \"./\"\ntrain_part.to_csv(f\"{save_path}/train.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\ntest_part.to_csv(f\"{save_path}/test.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\ndata_files = {\n    \"train\": \"./train.csv\", \n    \"validation\": \"./test.csv\",\n}\n\ndataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\", )\ntrain_dataset = dataset[\"train\"]\neval_dataset = dataset[\"validation\"]\n\nlabel_list = train_dataset.unique(output_column)\nlabel_list.sort() \nnum_labels = len(label_list)\n\n\n\nconfig = AutoConfig.from_pretrained(\n    model_name_or_path,\n    num_labels=num_labels,\n    label2id={label: i for i, label in enumerate(label_list)},\n    id2label={i: label for i, label in enumerate(label_list)},\n    finetuning_task=\"wav2vec2_clf\",\n)\nsetattr(config, 'pooling_mode', pooling_mode)\nprocessor = Wav2Vec2Processor.from_pretrained(model_name_or_path,)\ntarget_sampling_rate = processor.feature_extractor.sampling_rate\nprocessor = Wav2Vec2Processor.from_pretrained(model_name_or_path,)\ntarget_sampling_rate = processor.feature_extractor.sampling_rate\n\n\ntrain_dataset = train_dataset.map(\npreprocess_function,\nbatch_size=100,\nbatched=True,\nnum_proc=4\n)\neval_dataset = eval_dataset.map(\n    preprocess_function,\n    batch_size=100,\n    batched=True,\n    num_proc=4\n)\nprocessor = Wav2Vec2Processor.from_pretrained(model_name_or_path)\n\nprint(f\"Целевой сэмплинг рейт: {target_sampling_rate}\")\nprint(f\"Количество лейблов {num_labels}, классы : {label_list}\")\nprint(f\"Используется: {device}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:30:58.458426Z","iopub.execute_input":"2022-05-25T09:30:58.459254Z","iopub.status.idle":"2022-05-25T09:31:34.946701Z","shell.execute_reply.started":"2022-05-25T09:30:58.459189Z","shell.execute_reply":"2022-05-25T09:31:34.945355Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True) ","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:31:34.949887Z","iopub.execute_input":"2022-05-25T09:31:34.950408Z","iopub.status.idle":"2022-05-25T09:31:34.958433Z","shell.execute_reply.started":"2022-05-25T09:31:34.950344Z","shell.execute_reply":"2022-05-25T09:31:34.957234Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"eval_pd = eval_dataset.to_pandas()\ntrain_pd = train_dataset.to_pandas()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:31:34.960272Z","iopub.execute_input":"2022-05-25T09:31:34.960683Z","iopub.status.idle":"2022-05-25T09:31:36.271646Z","shell.execute_reply.started":"2022-05-25T09:31:34.960636Z","shell.execute_reply":"2022-05-25T09:31:36.270909Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"EMOTIONS = num_labels # Количество классов\n\n\ndistribution = torch.distributions.beta.Beta(torch.tensor([ALPHA]), torch.tensor([ALPHA])) #torch.rand(1).item()\n\n# Это один из примеров возможного смешивания, который не используется, но также был исследован при экспериментах \ndef permute_audio(audio, permutation):\n    return np.hstack(np.array(np.array_split(audio, len(permutation)),dtype=object)[permutation])\n\n# Функция смешивания двух образцов записи в соответствии с описанным в работе подходом\ndef mixup(audio1, audio2, label1, label2):\n    proportion = distribution.sample().item() # Семплируем из бета-распределения\n    take1 = round(len(audio1)*proportion) #  Берем соответствующие пропорции\n    take2 = round(len(audio2)*(1-proportion))\n    audio = np.hstack([audio1[:take1], audio2[-take2:]])\n    label = label1*proportion + label2*(1-proportion) # Изменяем метки соответствующим образом\n    return audio, label\n\n# Реализация датасета для получения записи из набора данных\n\nclass AudioDataset(Dataset):\n    def __init__(self, is_train, df, n_splits=4, p_augment = 0.5, use_augment=True): # possible use random n_splits every time\n        self.df = df\n        self.is_train = is_train\n        self.n_splits = n_splits\n        self.p_augment = p_augment\n        self.use_augment = use_augment\n        \n        \n    def __len__(self):\n        return self.df.shape[0]\n    \n    \n    def __getitem__(self, index):\n        item = self.df.iloc[index].to_dict()\n        if self.is_train and (np.random.uniform() < self.p_augment) and self.use_augment: \n            # С некоторой вероятностью аугментируем запись из тренировочного датасета\n            actor = item['actor']\n            same_actor = self.df.query(f'actor=={actor}') # Рассматриваем множетво записей того же актера без ограничения эмоций\n            pick = np.random.randint(0, same_actor.shape[0]) # Выбираем случайную запись этого актера\n            another_sample = self.df.iloc[pick].to_dict()\n            # Далее смешиваем\n            audio, sample = mixup(item['input_values'], another_sample['input_values'], \n                  torch.nn.functional.one_hot(torch.tensor(item['labels']), EMOTIONS),\n                  torch.nn.functional.one_hot(torch.tensor(another_sample['labels']), EMOTIONS)\n                 )\n            item['input_values'] = audio\n            item['labels'] = sample\n        else:\n            item['labels'] = torch.nn.functional.one_hot(torch.tensor(item['labels']), EMOTIONS).float()\n        return item","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:31:36.273277Z","iopub.execute_input":"2022-05-25T09:31:36.273516Z","iopub.status.idle":"2022-05-25T09:31:36.304813Z","shell.execute_reply.started":"2022-05-25T09:31:36.273486Z","shell.execute_reply":"2022-05-25T09:31:36.304040Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Создание экземпляров датасета \ntrainds = AudioDataset(True, train_pd, use_augment=AUGMENTED)\nevalds = AudioDataset(False, eval_pd)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:31:39.684251Z","iopub.execute_input":"2022-05-25T09:31:39.684626Z","iopub.status.idle":"2022-05-25T09:31:39.689522Z","shell.execute_reply.started":"2022-05-25T09:31:39.684583Z","shell.execute_reply":"2022-05-25T09:31:39.688689Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#  Содание загрузчика с использванием коллатора\ndata_collator = DataCollatorCTCWithPadding(processor=processor, padding=True) # \ntrain_dl = DataLoader(trainds, collate_fn=data_collator, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:31:40.056423Z","iopub.execute_input":"2022-05-25T09:31:40.057441Z","iopub.status.idle":"2022-05-25T09:31:40.064324Z","shell.execute_reply.started":"2022-05-25T09:31:40.057364Z","shell.execute_reply":"2022-05-25T09:31:40.063316Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Определение класса модели\nmodel = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_or_path)\nmodel.to(device)\nmodel.freeze_feature_extractor()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:31:42.627416Z","iopub.execute_input":"2022-05-25T09:31:42.628543Z","iopub.status.idle":"2022-05-25T09:32:22.551082Z","shell.execute_reply.started":"2022-05-25T09:31:42.628491Z","shell.execute_reply":"2022-05-25T09:32:22.550164Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Аргументы для huggingface trainer и обучение модели\ntraining_args = TrainingArguments(\n    output_dir=OUT_DIR,\n    per_device_train_batch_size=3, # Большее количество записей не помещалось\n    per_device_eval_batch_size=3,\n    gradient_accumulation_steps=2,\n    evaluation_strategy=\"steps\",\n    num_train_epochs=EPOCHS,\n    fp16=(True if device =='cuda' else False), # Оптимизация по памяти\n    save_steps=500, # Можно сохранять модель\n    eval_steps=20, # Как часто проводить валидацию\n    logging_steps=10,\n    learning_rate=1e-4,\n    save_total_limit=2,\n    lr_scheduler_type='polynomial' # Использую полиномиальное сокращение шага, как указано в работе\n)\n\ntrainer = CTCTrainer(\n    model=model,\n    data_collator=data_collator,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=trainds,\n    eval_dataset=evalds,\n    tokenizer=processor.feature_extractor\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T09:32:22.553252Z","iopub.execute_input":"2022-05-25T09:32:22.553648Z"},"trusted":true},"execution_count":null,"outputs":[]}]}