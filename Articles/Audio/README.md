<table>
  <tr>
    <th>Article</th>
    <th>Authors</th>
    <th>Link</th> 
    <th>Abstract</th>
  </tr>
  <tr>
    <td>AST: Audio Spectrogram Transformer</td>
    <td>Yuan Gong, Yu-An Chung, James Glass</td> 
    <td>https://arxiv.org/abs/2104.01778</td>
    <td>In the past decade, convolutional neural networks (CNNs) have been widely adopted as the main building block for end-to-end audio classification models, which aim to learn a direct mapping from audio spectrograms to corresponding labels. To better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it is unclear whether the reliance on a CNN is necessary, and if neural networks purely based on attention are sufficient to obtain good performance in audio classification. </td>
  </tr>
</table>
