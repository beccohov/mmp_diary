<table>
  <tr>
    <th>Article</th>
    <th>Authors</th>
    <th>Link</th> 
    <th>Abstract</th>
  </tr>
  <tr>
    <td>What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding</td>
    <td>Yu-An Wang, Yun-Nung Chen</td> 
    <td>https://arxiv.org/abs/2010.04903</td>
    <td>In recent years, pre-trained Transformers have dominated the majority of NLP benchmark tasks. Many variants of pre-trained Transformers have kept breaking out, and most focus on designing different pre-training objectives or variants of self-attention. Embedding the position information in the self-attention mechanism is also an indispensable factor in Transformers however is often discussed at will. Therefore, this paper carries out an empirical study on position embeddings of mainstream pre-trained Transformers, which mainly focuses on two questions: 1) Do position embeddings really learn the meaning of positions? 2) How do these different learned position embeddings affect Transformers for NLP tasks? </td>
  </tr>
  <tr id="current_issue">
    <td>Categorical Reparameterization with Gumbel-Softmax</td>
    <td>Eric Jang, Shixiang Gu, Ben Poole</td> 
    <td>https://arxiv.org/pdf/1611.01144.pdf</td>
    <td>Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.

 </td>
  </tr>
</table>
