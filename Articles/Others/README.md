<table>
  <tr>
    <th>Article</th>
    <th>Authors</th>
    <th>Link</th> 
    <th>Abstract</th>
  </tr>
  <tr>
    <td>What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding</td>
    <td>Yu-An Wang, Yun-Nung Chen</td> 
    <td>https://arxiv.org/abs/2010.04903</td>
    <td>In recent years, pre-trained Transformers have dominated the majority of NLP benchmark tasks. Many variants of pre-trained Transformers have kept breaking out, and most focus on designing different pre-training objectives or variants of self-attention. Embedding the position information in the self-attention mechanism is also an indispensable factor in Transformers however is often discussed at will. Therefore, this paper carries out an empirical study on position embeddings of mainstream pre-trained Transformers, which mainly focuses on two questions: 1) Do position embeddings really learn the meaning of positions? 2) How do these different learned position embeddings affect Transformers for NLP tasks? </td>
  </tr>
</table>
